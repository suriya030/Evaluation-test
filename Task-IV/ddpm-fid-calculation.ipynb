{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **0. Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\ddpm-pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imporing required libraries   \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import os\n",
    "from scipy import linalg\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom dataset ##\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, target_dir, transform=None):\n",
    "        self.paths = [ os.path.join(target_dir, x) for x in os.listdir(target_dir) if x.endswith('.npy')]\n",
    "        # load all the paths of numpy images from the target directory    \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.paths[index]\n",
    "        img = np.load(img_path) # (1,150,150)     \n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FID score computation ##\n",
    "\n",
    "def get_activation(dataloader, model, preprocess, # Preprocessing Transform for InceptionV3\n",
    "                   device = 'cpu'\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Given Dataloader and Model, Generate N X 2048\n",
    "    Dimensional activation map for N data points\n",
    "    in dataloader.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation Mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # Save activations\n",
    "    pred_arr = np.zeros((len(dataloader.dataset), 2048))\n",
    "    batch_size = dataloader.batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(dataloader)):\n",
    "            # Transform the Batch according to Inceptionv3 specification\n",
    "            batch = torch.stack([preprocess(img) for img in batch]).to(device)\n",
    "            pred = model(batch).cpu().numpy()\n",
    "            pred_arr[i*batch_size : i*batch_size + batch.size(0), :] = pred\n",
    "            \n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(dataloader, model, preprocess, device='cpu'):\n",
    "    \"\"\"\n",
    "    Get mean vector and covariance matrix of the activation maps.\n",
    "    \"\"\"\n",
    "    # Get activation maps\n",
    "    act = get_activation(dataloader, model, preprocess, device)\n",
    "    mu = np.mean(act, axis=0) # Mean\n",
    "    sigma = np.cov(act, rowvar=False) # Covariance  \n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "from scipy import linalg\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given Mean and Sigma of Real and Generated Data,\n",
    "    it calculates FID between them using:\n",
    "     \n",
    "     d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "     \n",
    "    \"\"\"\n",
    "    # Make sure they have appropriate dims\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # Handle various cases\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = (\n",
    "            \"fid calculation produces singular product; \"\n",
    "            \"adding %s to diagonal of cov estimates\"\n",
    "        ) % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fréchet Inception Distance**\n",
    "\n",
    "FID is a measure of similarity between two datasets of images. It was shown to correlate well with human judgement of visual quality and is most often used to evaluate the quality of samples of Generative Adversarial Networks. FID is calculated by computing the Fréchet distance between two Gaussians fitted to feature representations of the Inception network.\n",
    "\n",
    "Given **Mean** and **Sigma** of Real and Generated Data,  \n",
    "it calculates **FID** between them using:\n",
    "\n",
    "**d² = ‖μ₁ - μ₂‖² + Tr(C₁ + C₂ - 2·√(C₁·C₂))**\n",
    "\n",
    "Where:  \n",
    "- **μ₁**, **C₁**: Mean and covariance of real data  \n",
    "- **μ₂**, **C₂**: Mean and covariance of generated data  \n",
    "\n",
    "Run the cell below to calculate **FID** ⬇️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Dataset Size     : 9000\n",
      " Test Dataset Size      : 1000\n",
      " Generated Dataset Size : 10000\n",
      "\n",
      "Calculating Mean and Sigma for Train Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [00:26,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Mean and Sigma for Test Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:02,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Mean and Sigma for Generated Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:27,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID-Score (b/w train and generated data ): 10.537900226665158\n",
      "FID-Score (b/w test and generated data ): 11.65205665853604\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE' # could potentially produce incorrect numerical computations\n",
    "# but there is no better workaround for now.\n",
    "\n",
    "# loading config file\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config_path = r'config\\default.yaml'\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']\n",
    "dataset_config = config['dataset_params']\n",
    "diffusion_config = config['diffusion_params']\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((model_config['im_size'],model_config['im_size'])),\n",
    "                                transforms.Lambda(lambda x: x * 2 - 1)]) \n",
    "                                # Normalize to [-1, 1] ( similar to DDPM paper )\n",
    "\n",
    "train_dataset = CustomDataset(dataset_config['im_path'], transform=transform)   \n",
    "test_dataset = CustomDataset(dataset_config['test_im_path'], transform=transform)\n",
    "generated_dataset = CustomDataset(os.path.join(train_config['task_name'],dataset_config['generated_im_path'])\n",
    "                                  , transform=transform)\n",
    "\n",
    "print( f' Train Dataset Size     : {len(train_dataset)}')   \n",
    "print( f' Test Dataset Size      : {len(test_dataset)}')    \n",
    "print( f' Generated Dataset Size : {len(generated_dataset)}')      \n",
    "\n",
    "\n",
    "# Transform to Convert Output of CustomDataset class to Inception format.\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_inception = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: (x + 1.0)/2.0), # [-1, 1] => [0, 1]\n",
    "    transforms.ToPILImage(), # Tensor to PIL Image \n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB format\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "\n",
    "])\n",
    "\n",
    "# Load InceptionV3 Model\n",
    "import torchvision.models as models\n",
    "# from torchvision.models.inception import Inception_V3_Weights\n",
    "# model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
    "model = models.inception_v3(pretrained=True) # ImageNet 1K weights loaded ( confirmed from documentation)\n",
    "model.fc = nn.Identity()\n",
    "\n",
    "# Mean and Sigma for Train Data\n",
    "print( '\\nCalculating Mean and Sigma for Train Data...')        \n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 64 , shuffle=False)\n",
    "mu3, sigma3 = calculate_activation_statistics(train_dataloader, model, preprocess = transform_inception,\n",
    "                                               device=device)\n",
    "\n",
    "# Mean and Sigma for Test Data\n",
    "print( '\\nCalculating Mean and Sigma for Test Data...') \n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 64  , shuffle=False)\n",
    "mu2, sigma2 = calculate_activation_statistics(test_dataloader, model, preprocess = transform_inception,\n",
    "                                               device=device)\n",
    "\n",
    "# Mean and Sigma For Generated Data\n",
    "print( '\\nCalculating Mean and Sigma for Generated Data...')    \n",
    "generated_dataloader = DataLoader(generated_dataset, batch_size= 64 , shuffle=False)\n",
    "mu1, sigma1 = calculate_activation_statistics(generated_dataloader, model, preprocess = transform_inception, \n",
    "                                              device=device)\n",
    "\n",
    "\n",
    "# Calculate FID between Generated and Train Data\n",
    "fid = calculate_frechet_distance(mu1, sigma1, mu3, sigma3)\n",
    "print(f'FID-Score (b/w train and generated data ): {fid}')  ## standard in literature.\n",
    "\n",
    "\n",
    "# Calculate FID between Generated and Test Data\n",
    "fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "print(f'FID-Score (b/w test and generated data ): {fid}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Similar to the original paper \"Denoising Diffusion Probabilistic Models\" by Ho et al. (2020). We have computed FID score between train-generated data ( standard practise in literature to use train set) and between test-generated data. Also, for reliable computation of FID computation we need atleast 5000-10000 images minimum ( empherical evidence)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
